name: Proxy Scanner

on:
  schedule:
    # Run every 6 hours
    - cron: '0 */6 * * *'
  push:
    branches: [main, master]
    paths:
      - 'socks5-scanner/src/**'
      - 'socks5-scanner/config/**'
      - '.github/workflows/scan.yml'
  workflow_dispatch:
    inputs:
      concurrency:
        description: 'Concurrency level'
        required: false
        default: '100'

jobs:
  scan:
    runs-on: ubuntu-latest
    permissions:
      contents: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('socks5-scanner/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        working-directory: socks5-scanner
        run: |
          python -m pip install --upgrade pip
          pip install aiohttp requests pyyaml

      - name: Create output directories
        working-directory: socks5-scanner
        run: mkdir -p proxies results

      - name: Run proxy scan
        working-directory: socks5-scanner
        continue-on-error: true
        run: |
          python -c "
          import asyncio
          import json
          import os
          from datetime import datetime

          from src.async_scanner_v2 import AsyncScanner
          from src.core import ScanConfig
          from src.scanner import Socks5Scanner

          async def main():
              config = ScanConfig(
                  connect_timeout=5.0,
                  max_concurrent=int(os.environ.get('CONCURRENCY', '100'))
              )

              # Use all 20+ sources from scanner module
              sources = Socks5Scanner.DEFAULT_SOURCES
              print(f'Using {len(sources)} proxy sources')

              # Fetch proxies
              import aiohttp
              proxies = set()
              async with aiohttp.ClientSession() as session:
                  for url in sources:
                      try:
                          async with session.get(url, timeout=aiohttp.ClientTimeout(total=30), ssl=False) as resp:
                              if resp.status == 200:
                                  text = await resp.text()
                                  for line in text.strip().split('\n'):
                                      line = line.strip()
                                      if ':' in line and len(line) < 50:
                                          # Extract IP:port pattern
                                          parts = line.split()
                                          if parts:
                                              candidate = parts[0]
                                              if ':' in candidate:
                                                  proxies.add(candidate)
                          print(f'  {url[:50]}... -> {len(proxies)} total')
                      except Exception as e:
                          print(f'  Failed: {url[:50]}... ({e})')

              print(f'Collected {len(proxies)} unique proxies')

              if not proxies:
                  print('No proxies found')
                  return

              # Scan
              async with AsyncScanner(config) as scanner:
                  results = await scanner.scan_many(list(proxies))

              # Save results
              working = results.get_working()
              valid = [r for r in results.results if r.socks5_valid]

              print(f'Results: {results.total} scanned, {len(valid)} valid, {len(working)} working')

              # Enrich with GeoIP data using a new session
              print('Enriching with GeoIP data...')
              enriched = []
              otx_key = os.environ.get('OTX_API_KEY', '')
              async with aiohttp.ClientSession() as geo_session:
                  for i, r in enumerate(working[:500]):  # Limit to 500 to avoid rate limits
                      proxy_data = r.to_dict()
                      ip = r.host or r.proxy.split(':')[0]

                      # Try to get geo data from free API (with lat/lon for map)
                      try:
                          async with geo_session.get(
                              f'http://ip-api.com/json/{ip}?fields=status,country,countryCode,city,lat,lon,isp,org,as',
                              timeout=aiohttp.ClientTimeout(total=5)
                          ) as geo_resp:
                              if geo_resp.status == 200:
                                  geo = await geo_resp.json()
                                  if geo.get('status') == 'success':
                                      proxy_data['geo'] = {
                                          'country': geo.get('country'),
                                          'country_code': geo.get('countryCode'),
                                          'city': geo.get('city'),
                                          'lat': geo.get('lat'),
                                          'lon': geo.get('lon'),
                                          'isp': geo.get('isp'),
                                          'org': geo.get('org'),
                                          'asn': geo.get('as', '').split()[0] if geo.get('as') else None,
                                          'asn_org': ' '.join(geo.get('as', '').split()[1:]) if geo.get('as') else None,
                                      }
                          await asyncio.sleep(0.15)  # Rate limit: 45/min
                      except Exception as e:
                          pass

                      # Optional: OTX threat intelligence (if API key provided)
                      if otx_key and i < 100:  # Limit OTX lookups
                          try:
                              headers = {'X-OTX-API-KEY': otx_key}
                              async with geo_session.get(
                                  f'https://otx.alienvault.com/api/v1/indicators/IPv4/{ip}/general',
                                  headers=headers,
                                  timeout=aiohttp.ClientTimeout(total=5)
                              ) as otx_resp:
                                  if otx_resp.status == 200:
                                      otx = await otx_resp.json()
                                      pulse_count = otx.get('pulse_info', {}).get('count', 0)
                                      proxy_data['threat'] = {
                                          'score': min(pulse_count, 10),
                                          'pulses': pulse_count,
                                          'tags': otx.get('pulse_info', {}).get('pulses', [])[:3]
                                      }
                              await asyncio.sleep(0.1)
                          except:
                              pass

                      enriched.append(proxy_data)
                      if (i + 1) % 50 == 0:
                          print(f'  GeoIP: {i + 1}/500 enriched')

              # Add remaining without geo (if > 500)
              for r in working[500:]:
                  enriched.append(r.to_dict())

              print(f'Enriched {len([e for e in enriched if e.get(\"geo\")])} proxies with geo data')

              # Save working proxies
              with open('proxies/proxies_working.txt', 'w') as f:
                  for r in working:
                      f.write(f'{r.proxy}\n')

              # Save valid proxies
              with open('proxies/proxies_valid.txt', 'w') as f:
                  for r in valid:
                      f.write(f'{r.proxy}\n')

              # Save full results as JSON with geo data
              with open('proxies/results.json', 'w') as f:
                  json.dump({
                      'timestamp': datetime.utcnow().isoformat() + 'Z',
                      'stats': {
                          'total': results.total,
                          'valid': len(valid),
                          'working': len(working),
                      },
                      'proxies': enriched
                  }, f, indent=2, default=str)

          asyncio.run(main())
          "
        env:
          CONCURRENCY: ${{ github.event.inputs.concurrency || '100' }}
          OTX_API_KEY: ${{ secrets.OTX_API_KEY }}

      - name: Generate summary
        working-directory: socks5-scanner
        continue-on-error: true
        run: |
          echo "## Proxy Scan Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          if [ -f proxies/results.json ]; then
            TOTAL=$(python -c "import json; d=json.load(open('proxies/results.json')); print(d.get('stats',{}).get('total', 0))" 2>/dev/null || echo "0")
            VALID=$(python -c "import json; d=json.load(open('proxies/results.json')); print(d.get('stats',{}).get('valid', 0))" 2>/dev/null || echo "0")
            WORKING=$(python -c "import json; d=json.load(open('proxies/results.json')); print(d.get('stats',{}).get('working', 0))" 2>/dev/null || echo "0")
            echo "| Metric | Count |" >> $GITHUB_STEP_SUMMARY
            echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
            echo "| Total Scanned | $TOTAL |" >> $GITHUB_STEP_SUMMARY
            echo "| SOCKS5 Valid | $VALID |" >> $GITHUB_STEP_SUMMARY
            echo "| Fully Working | $WORKING |" >> $GITHUB_STEP_SUMMARY
          else
            echo "No results file found" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Commit results
        working-directory: socks5-scanner
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          git add proxies/ || true
          git diff --staged --quiet || git commit -m "Update proxy list [skip ci]"
          git push || true
