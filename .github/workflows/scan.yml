name: Proxy Scanner

on:
  schedule:
    # Run every 6 hours
    - cron: '0 */6 * * *'
  push:
    branches: [main, master]
    paths:
      - 'socks5-scanner/src/**'
      - 'socks5-scanner/config/**'
      - '.github/workflows/scan.yml'
  workflow_dispatch:
    inputs:
      concurrency:
        description: 'Concurrency level'
        required: false
        default: '500'

jobs:
  scan:
    runs-on: ubuntu-latest
    permissions:
      contents: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('socks5-scanner/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        working-directory: socks5-scanner
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Create output directories
        working-directory: socks5-scanner
        run: mkdir -p proxies results

      - name: Run proxy scan
        working-directory: socks5-scanner
        continue-on-error: true
        timeout-minutes: 45
        run: |
          python -c "
          import asyncio
          import json
          import os
          import sys
          from datetime import datetime

          # Use uvloop for faster async (20-30% speedup)
          try:
              import uvloop
              asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())
              print('Using uvloop for faster async')
          except ImportError:
              print('uvloop not available, using default event loop')

          import aiohttp

          from src.async_scanner_v2 import AsyncScanner
          from src.core import ScanConfig
          from src.scanner import Socks5Scanner

          async def fetch_source(session, url, proxies_set):
              \"\"\"Fetch a single source and add to proxy set.\"\"\"
              try:
                  async with session.get(url, timeout=aiohttp.ClientTimeout(total=20), ssl=False) as resp:
                      if resp.status == 200:
                          text = await resp.text()
                          count_before = len(proxies_set)
                          for line in text.strip().split('\n'):
                              line = line.strip()
                              if ':' in line and len(line) < 50:
                                  parts = line.split()
                                  if parts:
                                      candidate = parts[0]
                                      if ':' in candidate:
                                          proxies_set.add(candidate)
                          added = len(proxies_set) - count_before
                          return (url[:40], added, None)
              except Exception as e:
                  return (url[:40], 0, str(e)[:30])
              return (url[:40], 0, 'no data')

          async def batch_geoip(session, ips):
              \"\"\"Batch GeoIP lookup - 100 IPs per request (much faster).\"\"\"
              results = dict()
              try:
                  # ip-api.com batch endpoint expects list of query objects
                  batch_data = [dict(query=ip, fields='status,query,country,countryCode,city,lat,lon,isp,org,as') for ip in ips[:100]]
                  async with session.post(
                      'http://ip-api.com/batch',
                      json=batch_data,
                      timeout=aiohttp.ClientTimeout(total=15)
                  ) as resp:
                      if resp.status == 200:
                          data = await resp.json()
                          for item in data:
                              if item.get('status') == 'success':
                                  ip = item.get('query')
                                  as_field = item.get('as', '')
                                  results[ip] = dict(
                                      country=item.get('country'),
                                      country_code=item.get('countryCode'),
                                      city=item.get('city'),
                                      lat=item.get('lat'),
                                      lon=item.get('lon'),
                                      isp=item.get('isp'),
                                      org=item.get('org'),
                                      asn=as_field.split()[0] if as_field else None,
                                      asn_org=' '.join(as_field.split()[1:]) if as_field else None,
                                  )
                      else:
                          print(f'  Batch GeoIP HTTP {resp.status}')
              except Exception as e:
                  print(f'  Batch GeoIP error: {e}')
              return results

          async def fetch_blocklists(session):
              \"\"\"Fetch threat intelligence blocklists (free, unlimited).\"\"\"
              blocklists = dict(
                  feodo=set(),      # Botnet C2 IPs
                  sslbl=set(),      # SSL abusers
                  urlhaus=set(),    # Malware distribution
              )

              sources = [
                  ('feodo', 'https://feodotracker.abuse.ch/downloads/ipblocklist.txt'),
                  ('sslbl', 'https://sslbl.abuse.ch/blacklist/sslipblacklist.txt'),
                  ('urlhaus', 'https://urlhaus.abuse.ch/downloads/text/'),
              ]

              for name, url in sources:
                  try:
                      async with session.get(url, timeout=aiohttp.ClientTimeout(total=30)) as resp:
                          if resp.status == 200:
                              text = await resp.text()
                              for line in text.split('\\n'):
                                  line = line.strip()
                                  if line and not line.startswith('#') and not line.startswith(';'):
                                      # Extract IP from various formats
                                      if '/' in line:  # CIDR notation - just use base IP
                                          ip = line.split('/')[0]
                                      elif ':' in line and 'http' not in line.lower():  # IP:port
                                          ip = line.split(':')[0]
                                      elif 'http' in line.lower():  # URL - extract host
                                          import re
                                          match = re.search(r'://([0-9.]+)', line)
                                          if match:
                                              ip = match.group(1)
                                          else:
                                              continue
                                      else:
                                          ip = line
                                      # Validate it looks like an IP
                                      parts = ip.split('.')
                                      if len(parts) == 4 and all(p.isdigit() for p in parts):
                                          blocklists[name].add(ip)
                              print(f'  {name}: {len(blocklists[name])} IPs loaded')
                  except Exception as e:
                      print(f'  {name}: failed ({str(e)[:30]})')

              return blocklists

          def check_blocklists(ip, blocklists):
              \"\"\"Check IP against all blocklists, return threat info.\"\"\"
              sources = []
              for name, ips in blocklists.items():
                  if ip in ips:
                      sources.append(name)
              if sources:
                  return dict(
                      score=min(len(sources) * 5, 10),
                      sources=sources,
                      flagged=True,
                  )
              return None

          async def main():
              # Validate concurrency input (min: 10, max: 2000)
              concurrency = int(os.environ.get('CONCURRENCY', '500'))
              concurrency = max(10, min(2000, concurrency))
              config = ScanConfig(
                  connect_timeout=3.0,  # Reduced from 5s
                  read_timeout=3.0,
                  max_concurrent=concurrency
              )

              sources = Socks5Scanner.DEFAULT_SOURCES
              print(f'Using {len(sources)} proxy sources with concurrency={concurrency}')

              # OPTIMIZATION 1: Parallel source fetching
              print('Fetching sources in parallel...')
              proxies = set()
              async with aiohttp.ClientSession() as session:
                  tasks = [fetch_source(session, url, proxies) for url in sources]
                  results = await asyncio.gather(*tasks, return_exceptions=True)
                  for r in results:
                      if isinstance(r, tuple):
                          url, added, err = r
                          if err:
                              print(f'  {url}... failed: {err}')
                          elif added > 0:
                              print(f'  {url}... +{added}')

              print(f'Collected {len(proxies)} unique proxies')

              if not proxies:
                  print('No proxies found')
                  return

              # Scan with higher concurrency
              print(f'Scanning {len(proxies)} proxies with {concurrency} concurrent...')
              start_time = datetime.now()

              async with AsyncScanner(config) as scanner:
                  results = await scanner.scan_many(list(proxies))

              scan_time = (datetime.now() - start_time).total_seconds()
              print(f'Scan completed in {scan_time:.1f}s ({len(proxies)/scan_time:.0f} proxies/sec)')

              working = results.get_working()
              valid = [r for r in results.results if r.socks5_valid]

              print(f'Results: {results.total} scanned, {len(valid)} valid, {len(working)} working')

              # OPTIMIZATION 2: Batch GeoIP (100 IPs per request instead of 1)
              print('Enriching with GeoIP data (batch mode)...')
              enriched = []
              geo_limit = min(len(working), 500)

              async with aiohttp.ClientSession() as geo_session:
                  # Extract IPs for batch lookup
                  ips_to_lookup = []
                  for r in working[:geo_limit]:
                      ip = r.host or r.proxy.split(':')[0]
                      ips_to_lookup.append(ip)

                  # Batch lookup in chunks of 100
                  all_geo = {}
                  for i in range(0, len(ips_to_lookup), 100):
                      batch = ips_to_lookup[i:i+100]
                      geo_results = await batch_geoip(geo_session, batch)
                      all_geo.update(geo_results)
                      print(f'  GeoIP batch {i//100 + 1}/{(len(ips_to_lookup)+99)//100}: {len(geo_results)} results')
                      await asyncio.sleep(0.5)  # Rate limit between batches

                  # Fetch threat blocklists (free, unlimited - checks ALL proxies)
                  print('Fetching threat intelligence blocklists...')
                  blocklists = await fetch_blocklists(geo_session)
                  total_blocklist_ips = sum(len(v) for v in blocklists.values())
                  print(f'Total blocklist IPs: {total_blocklist_ips}')

                  # Build enriched list with threat checks
                  otx_key = os.environ.get('OTX_API_KEY', '')
                  otx_success = 0
                  otx_failed = 0
                  otx_limit = 50  # OTX has rate limits, so only check first 50
                  blocklist_hits = 0

                  if otx_key:
                      print(f'OTX API key found, will check first {otx_limit} proxies')

                  for i, r in enumerate(working[:geo_limit]):
                      proxy_data = r.to_dict()
                      ip = r.host or r.proxy.split(':')[0]

                      if ip in all_geo:
                          proxy_data['geo'] = all_geo[ip]

                      # Check against blocklists (ALL proxies, no rate limit)
                      threat_data = check_blocklists(ip, blocklists)
                      if threat_data:
                          proxy_data['threat'] = threat_data
                          blocklist_hits += 1
                      else:
                          # Default to clean if not in blocklists
                          proxy_data['threat'] = dict(score=0, sources=[], flagged=False)

                      # Optional OTX enrichment (adds pulse count for first 50)
                      if otx_key and i < otx_limit:
                          try:
                              headers = dict()
                              headers['X-OTX-API-KEY'] = otx_key
                              async with geo_session.get(
                                  f'https://otx.alienvault.com/api/v1/indicators/IPv4/{ip}/general',
                                  headers=headers,
                                  timeout=aiohttp.ClientTimeout(total=5)
                              ) as otx_resp:
                                  if otx_resp.status == 200:
                                      otx = await otx_resp.json()
                                      pulse_count = otx.get('pulse_info', dict()).get('count', 0)
                                      # Merge OTX data with blocklist data
                                      proxy_data['threat']['pulses'] = pulse_count
                                      if pulse_count > 0:
                                          proxy_data['threat']['score'] = max(proxy_data['threat'].get('score', 0), min(pulse_count, 10))
                                          if 'otx' not in proxy_data['threat'].get('sources', []):
                                              proxy_data['threat'].setdefault('sources', []).append('otx')
                                      otx_success += 1
                                  elif otx_resp.status == 403:
                                      if otx_failed == 0:
                                          print(f'  OTX API key invalid or expired (HTTP 403)')
                                      otx_failed += 1
                                  elif otx_resp.status == 429:
                                      if otx_failed == 0:
                                          print(f'  OTX rate limit exceeded (HTTP 429)')
                                      otx_failed += 1
                                  else:
                                      otx_failed += 1
                          except Exception as e:
                              if otx_failed == 0:
                                  print(f'  OTX lookup error: {str(e)[:50]}')
                              otx_failed += 1

                      enriched.append(proxy_data)

                  print(f'Threat check: {blocklist_hits} flagged in blocklists')
                  if otx_key:
                      print(f'OTX lookups: {otx_success} successful, {otx_failed} failed')

              # Add remaining without geo
              for r in working[geo_limit:]:
                  enriched.append(r.to_dict())

              geo_count = len([e for e in enriched if e.get('geo')])
              threat_count = len([e for e in enriched if e.get('threat')])
              print(f'Enriched {geo_count} proxies with geo data, {threat_count} with threat data')

              # Save results
              with open('proxies/proxies_working.txt', 'w') as f:
                  for r in working:
                      f.write(f'{r.proxy}\n')

              with open('proxies/proxies_valid.txt', 'w') as f:
                  for r in valid:
                      f.write(f'{r.proxy}\n')

              with open('proxies/results.json', 'w') as f:
                  json.dump(dict(
                      timestamp=datetime.utcnow().isoformat() + 'Z',
                      stats=dict(
                          total=results.total,
                          valid=len(valid),
                          working=len(working),
                          geo_enriched=geo_count,
                          threat_checked=threat_count,
                          scan_time_sec=scan_time,
                      ),
                      proxies=enriched
                  ), f, indent=2, default=str)

              print(f'Done! Total time: {(datetime.now() - start_time).total_seconds():.1f}s')

          asyncio.run(main())
          "
        env:
          CONCURRENCY: ${{ github.event.inputs.concurrency || '500' }}
          OTX_API_KEY: ${{ secrets.OTX_API_KEY }}

      - name: Generate summary
        working-directory: socks5-scanner
        continue-on-error: true
        run: |
          echo "## Proxy Scan Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          if [ -f proxies/results.json ]; then
            TOTAL=$(python -c "import json; d=json.load(open('proxies/results.json')); print(d.get('stats',{}).get('total', 0))" 2>/dev/null || echo "0")
            VALID=$(python -c "import json; d=json.load(open('proxies/results.json')); print(d.get('stats',{}).get('valid', 0))" 2>/dev/null || echo "0")
            WORKING=$(python -c "import json; d=json.load(open('proxies/results.json')); print(d.get('stats',{}).get('working', 0))" 2>/dev/null || echo "0")
            GEO=$(python -c "import json; d=json.load(open('proxies/results.json')); print(d.get('stats',{}).get('geo_enriched', 0))" 2>/dev/null || echo "0")
            THREAT=$(python -c "import json; d=json.load(open('proxies/results.json')); print(d.get('stats',{}).get('threat_checked', 0))" 2>/dev/null || echo "0")
            TIME=$(python -c "import json; d=json.load(open('proxies/results.json')); print(f\"{d.get('stats',{}).get('scan_time_sec', 0):.0f}s\")" 2>/dev/null || echo "?")
            echo "| Metric | Count |" >> $GITHUB_STEP_SUMMARY
            echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
            echo "| Total Scanned | $TOTAL |" >> $GITHUB_STEP_SUMMARY
            echo "| SOCKS5 Valid | $VALID |" >> $GITHUB_STEP_SUMMARY
            echo "| Fully Working | $WORKING |" >> $GITHUB_STEP_SUMMARY
            echo "| GeoIP Enriched | $GEO |" >> $GITHUB_STEP_SUMMARY
            echo "| OTX Threat Data | $THREAT |" >> $GITHUB_STEP_SUMMARY
            echo "| Scan Time | $TIME |" >> $GITHUB_STEP_SUMMARY
          else
            echo "No results file found" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Commit results
        working-directory: socks5-scanner
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          git add proxies/ || true
          git diff --staged --quiet || git commit -m "Update proxy list [skip ci]"
          git push || true
