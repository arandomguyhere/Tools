name: Proxy Scanner

on:
  schedule:
    # Run every 6 hours
    - cron: '0 */6 * * *'
  push:
    branches: [main, master]
    paths:
      - 'socks5-scanner/src/**'
      - 'socks5-scanner/config/**'
      - '.github/workflows/scan.yml'
  workflow_dispatch:
    inputs:
      concurrency:
        description: 'Concurrency level'
        required: false
        default: '500'

jobs:
  scan:
    runs-on: ubuntu-latest
    permissions:
      contents: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('socks5-scanner/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        working-directory: socks5-scanner
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          # Install geoip2 for offline GeoIP (50K+ lookups/sec, no rate limits)
          pip install geoip2 maxminddb

      - name: Download GeoLite2 database
        working-directory: socks5-scanner
        run: |
          mkdir -p data
          # Download free GeoLite2-City database (updated daily, no account needed)
          curl -sL "https://github.com/P3TERX/GeoLite.mmdb/raw/download/GeoLite2-City.mmdb" -o data/GeoLite2-City.mmdb
          # Download free GeoLite2-ASN database for ASN/Org data
          curl -sL "https://github.com/P3TERX/GeoLite.mmdb/raw/download/GeoLite2-ASN.mmdb" -o data/GeoLite2-ASN.mmdb
          ls -lh data/

      - name: Create output directories
        working-directory: socks5-scanner
        run: mkdir -p proxies results

      - name: Run proxy scan
        working-directory: socks5-scanner
        continue-on-error: true
        timeout-minutes: 90
        run: |
          python -c "
          import asyncio
          import json
          import os
          import sys
          from datetime import datetime

          # Use uvloop for faster async (20-30% speedup)
          try:
              import uvloop
              asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())
              print('Using uvloop for faster async')
          except ImportError:
              print('uvloop not available, using default event loop')

          import aiohttp

          from src.async_scanner_v2 import AsyncScanner
          from src.core import ScanConfig
          from src.scanner import Socks5Scanner

          # Initialize geoip2 with free GeoLite2 databases (50K+ lookups/sec)
          # Falls back gracefully if databases not available
          geoip_city_reader = None
          geoip_asn_reader = None
          try:
              import geoip2.database
              city_path = 'data/GeoLite2-City.mmdb'
              asn_path = 'data/GeoLite2-ASN.mmdb'
              geoip_city_reader = geoip2.database.Reader(city_path)
              print(f'Loaded GeoLite2-City database from {city_path}')
              try:
                  geoip_asn_reader = geoip2.database.Reader(asn_path)
                  print(f'Loaded GeoLite2-ASN database from {asn_path}')
              except Exception as e:
                  print(f'GeoLite2-ASN database not available: {e}')
          except Exception as e:
              print(f'GeoLite2 database not available, will use ip-api.com only: {e}')

          def offline_geoip_lookup(ip):
              \"\"\"Fast offline GeoIP lookup using geoip2 (~50K lookups/sec).\"\"\"
              if not geoip_city_reader:
                  return None
              try:
                  result = geoip_city_reader.city(ip)
                  if result and result.country.iso_code:
                      geo_data = dict(
                          country=result.country.name,
                          country_code=result.country.iso_code,
                          city=result.city.name,
                          lat=result.location.latitude,
                          lon=result.location.longitude,
                          asn=None,
                          asn_org=None,
                          isp=None,
                          org=None,
                          source='geolite2',
                      )
                      # Add ASN data if available
                      if geoip_asn_reader:
                          try:
                              asn_result = geoip_asn_reader.asn(ip)
                              if asn_result:
                                  geo_data['asn'] = f'AS{asn_result.autonomous_system_number}'
                                  geo_data['asn_org'] = asn_result.autonomous_system_organization
                                  geo_data['org'] = asn_result.autonomous_system_organization
                          except Exception:
                              pass
                      return geo_data
              except Exception:
                  pass
              return None

          async def fetch_source(session, url, proxies_set):
              \"\"\"Fetch a single source and add to proxy set.\"\"\"
              try:
                  async with session.get(url, timeout=aiohttp.ClientTimeout(total=20), ssl=False) as resp:
                      if resp.status == 200:
                          text = await resp.text()
                          count_before = len(proxies_set)
                          for line in text.strip().split('\n'):
                              line = line.strip()
                              if ':' in line and len(line) < 50:
                                  parts = line.split()
                                  if parts:
                                      candidate = parts[0]
                                      if ':' in candidate:
                                          proxies_set.add(candidate)
                          added = len(proxies_set) - count_before
                          return (url[:40], added, None)
              except Exception as e:
                  return (url[:40], 0, str(e)[:30])
              return (url[:40], 0, 'no data')

          async def batch_geoip(session, ips):
              \"\"\"Batch GeoIP lookup - 100 IPs per request (much faster).\"\"\"
              results = dict()
              try:
                  # ip-api.com batch endpoint expects list of query objects
                  batch_data = [dict(query=ip, fields='status,query,country,countryCode,city,lat,lon,isp,org,as') for ip in ips[:100]]
                  async with session.post(
                      'http://ip-api.com/batch',
                      json=batch_data,
                      timeout=aiohttp.ClientTimeout(total=15)
                  ) as resp:
                      if resp.status == 200:
                          data = await resp.json()
                          for item in data:
                              if item.get('status') == 'success':
                                  ip = item.get('query')
                                  as_field = item.get('as', '')
                                  results[ip] = dict(
                                      country=item.get('country'),
                                      country_code=item.get('countryCode'),
                                      city=item.get('city'),
                                      lat=item.get('lat'),
                                      lon=item.get('lon'),
                                      isp=item.get('isp'),
                                      org=item.get('org'),
                                      asn=as_field.split()[0] if as_field else None,
                                      asn_org=' '.join(as_field.split()[1:]) if as_field else None,
                                  )
                      else:
                          print(f'  Batch GeoIP HTTP {resp.status}')
              except Exception as e:
                  print(f'  Batch GeoIP error: {e}')
              return results

          # Import threat module (proper CIDR support, multiple sources, better scoring)
          from src.threat import ThreatChecker

          async def main():
              # Validate concurrency input (min: 10, max: 2000)
              concurrency = int(os.environ.get('CONCURRENCY', '500'))
              concurrency = max(10, min(2000, concurrency))
              config = ScanConfig(
                  connect_timeout=3.0,  # Reduced from 5s
                  read_timeout=3.0,
                  max_concurrent=concurrency
              )

              sources = Socks5Scanner.DEFAULT_SOURCES
              print(f'Using {len(sources)} proxy sources with concurrency={concurrency}')

              # OPTIMIZATION 1: Parallel source fetching
              print('Fetching sources in parallel...')
              proxies = set()
              async with aiohttp.ClientSession() as session:
                  tasks = [fetch_source(session, url, proxies) for url in sources]
                  results = await asyncio.gather(*tasks, return_exceptions=True)
                  for r in results:
                      if isinstance(r, tuple):
                          url, added, err = r
                          if err:
                              print(f'  {url}... failed: {err}')
                          elif added > 0:
                              print(f'  {url}... +{added}')

              print(f'Collected {len(proxies)} unique proxies')

              if not proxies:
                  print('No proxies found')
                  return

              # Scan with higher concurrency
              print(f'Scanning {len(proxies)} proxies with {concurrency} concurrent...')
              start_time = datetime.now()

              async with AsyncScanner(config) as scanner:
                  results = await scanner.scan_many(list(proxies))

              scan_time = (datetime.now() - start_time).total_seconds()
              print(f'Scan completed in {scan_time:.1f}s ({len(proxies)/scan_time:.0f} proxies/sec)')

              working = results.get_working()
              valid = [r for r in results.results if r.socks5_valid]

              print(f'Results: {results.total} scanned, {len(valid)} valid, {len(working)} working')

              # HYBRID GEOIP: Offline first (100K+/sec), then API fallback
              # Phase 1: Fast offline lookup for ALL working proxies
              print('Enriching with GeoIP data (hybrid: offline + API fallback)...')
              enriched = []
              all_geo = {}
              offline_hits = 0
              offline_misses = []

              # Extract all IPs
              all_ips = []
              for r in working:
                  ip = r.host or r.proxy.split(':')[0]
                  all_ips.append(ip)

              # Phase 1: Offline lookup (instant, no rate limits)
              if geoip_city_reader:
                  print(f'  Phase 1: Offline lookup for {len(all_ips)} IPs...')
                  import time
                  start = time.time()
                  for ip in all_ips:
                      geo = offline_geoip_lookup(ip)
                      if geo:
                          all_geo[ip] = geo
                          offline_hits += 1
                      else:
                          offline_misses.append(ip)
                  elapsed = time.time() - start
                  print(f'  Offline: {offline_hits} found, {len(offline_misses)} missing ({elapsed:.2f}s, {len(all_ips)/max(elapsed,0.001):.0f} lookups/sec)')
              else:
                  offline_misses = all_ips
                  print(f'  Offline DB not available, using API for all {len(all_ips)} IPs')

              # Phase 2: API fallback for missing IPs (rate limited)
              async with aiohttp.ClientSession() as geo_session:
                  if offline_misses:
                      print(f'  Phase 2: API fallback for {len(offline_misses)} IPs...')
                      for i in range(0, len(offline_misses), 100):
                          batch = offline_misses[i:i+100]
                          geo_results = await batch_geoip(geo_session, batch)
                          # Mark API results with source
                          for ip, geo in geo_results.items():
                              geo['source'] = 'ip-api.com'
                          all_geo.update(geo_results)
                          print(f'    API batch {i//100 + 1}/{(len(offline_misses)+99)//100}: {len(geo_results)} results')
                          await asyncio.sleep(0.5)  # Rate limit between batches

                  # Fetch threat blocklists using threat module
                  # (proper CIDR support, 9 sources, severity-based scoring)
                  print('Fetching threat intelligence blocklists...')
                  threat_checker = ThreatChecker()
                  threat_checker._session = geo_session  # Reuse existing session
                  threat_stats = await threat_checker.fetch_blocklists()
                  print(f'  Sources: {threat_stats["sources_ok"]}/{threat_stats["sources_total"]} loaded')
                  print(f'  Total IPs: {threat_stats["total_ips"]}, Networks: {threat_stats["total_networks"]}')
                  for name, detail in threat_stats.get('details', {}).items():
                      if 'error' in detail:
                          print(f'    {name}: FAILED ({detail["error"]})')
                      else:
                          print(f'    {name}: {detail.get("ips", 0)} IPs, {detail.get("networks", 0)} networks')

                  # Build enriched list with threat checks
                  otx_key = os.environ.get('OTX_API_KEY', '')
                  otx_success = 0
                  otx_failed = 0
                  otx_limit = 50  # OTX has rate limits, so only check first 50
                  blocklist_hits = 0

                  if otx_key:
                      print(f'OTX API key found, will check first {otx_limit} proxies')

                  for i, r in enumerate(working):
                      proxy_data = r.to_dict()
                      ip = r.host or r.proxy.split(':')[0]

                      if ip in all_geo:
                          proxy_data['geo'] = all_geo[ip]

                      # Check against blocklists using threat module
                      # (proper CIDR matching, severity-based scoring)
                      threat_result = threat_checker.check_ip(ip)
                      proxy_data['threat'] = threat_result.to_dict()
                      if threat_result.flagged:
                          blocklist_hits += 1

                      # Optional OTX enrichment (adds pulse count for first 50)
                      if otx_key and i < otx_limit:
                          try:
                              headers = dict()
                              headers['X-OTX-API-KEY'] = otx_key
                              async with geo_session.get(
                                  f'https://otx.alienvault.com/api/v1/indicators/IPv4/{ip}/general',
                                  headers=headers,
                                  timeout=aiohttp.ClientTimeout(total=5)
                              ) as otx_resp:
                                  if otx_resp.status == 200:
                                      otx = await otx_resp.json()
                                      pulse_count = otx.get('pulse_info', dict()).get('count', 0)
                                      # Merge OTX data with blocklist data
                                      proxy_data['threat']['pulses'] = pulse_count
                                      if pulse_count > 0:
                                          new_score = max(proxy_data['threat'].get('score', 0), min(pulse_count, 10))
                                          proxy_data['threat']['score'] = new_score
                                          # Update level based on new score
                                          if new_score >= 5:
                                              proxy_data['threat']['level'] = 'risk'
                                          elif new_score > 0:
                                              proxy_data['threat']['level'] = 'low'
                                          if 'otx' not in proxy_data['threat'].get('sources', []):
                                              proxy_data['threat'].setdefault('sources', []).append('otx')
                                      otx_success += 1
                                  elif otx_resp.status == 403:
                                      if otx_failed == 0:
                                          print(f'  OTX API key invalid or expired (HTTP 403)')
                                      otx_failed += 1
                                  elif otx_resp.status == 429:
                                      if otx_failed == 0:
                                          print(f'  OTX rate limit exceeded (HTTP 429)')
                                      otx_failed += 1
                                  else:
                                      otx_failed += 1
                          except Exception as e:
                              if otx_failed == 0:
                                  print(f'  OTX lookup error: {str(e)[:50]}')
                              otx_failed += 1

                      enriched.append(proxy_data)

                  print(f'Threat check: {blocklist_hits} flagged in blocklists')
                  if otx_key:
                      print(f'OTX lookups: {otx_success} successful, {otx_failed} failed')

              geo_count = len([e for e in enriched if e.get('geo')])
              offline_count = len([e for e in enriched if e.get('geo', {}).get('source') == 'geolite2'])
              api_count = len([e for e in enriched if e.get('geo', {}).get('source') == 'ip-api.com'])
              threat_count = len([e for e in enriched if e.get('threat')])
              print(f'GeoIP: {geo_count} total ({offline_count} offline, {api_count} API)')
              print(f'Threat: {threat_count} proxies checked')

              # UDP ASSOCIATE testing (unique SOCKS5 feature most scanners skip)
              print('Testing UDP ASSOCIATE support (RFC 1928 CMD=0x03)...')
              try:
                  from src.udp_associate import test_udp_associate, UDPTestResult

                  udp_semaphore = asyncio.Semaphore(100)  # Limit concurrent UDP tests
                  udp_results = {}
                  udp_supported_count = 0
                  udp_not_supported = 0
                  udp_errors = 0

                  async def test_udp(proxy_str):
                      async with udp_semaphore:
                          try:
                              host, port_str = proxy_str.rsplit(':', 1)
                              port = int(port_str)
                              result = await test_udp_associate(host, port, timeout=3.0, test_dns=False)
                              return (proxy_str, result)
                          except Exception as e:
                              return (proxy_str, None)

                  # Test all working proxies for UDP support
                  proxy_list = [r.proxy for r in working]
                  print(f'  Testing {len(proxy_list)} proxies for UDP support...')

                  import time
                  udp_start = time.time()
                  udp_tasks = [test_udp(p) for p in proxy_list]
                  udp_task_results = await asyncio.gather(*udp_tasks)
                  udp_elapsed = time.time() - udp_start

                  for proxy_str, result in udp_task_results:
                      if result:
                          udp_results[proxy_str] = result.to_dict()
                          if result.udp_supported:
                              udp_supported_count += 1
                          elif result.result == UDPTestResult.NOT_SUPPORTED:
                              udp_not_supported += 1
                          else:
                              udp_errors += 1
                      else:
                          udp_errors += 1

                  # Add UDP results to enriched data
                  for e in enriched:
                      proxy = e.get('proxy')
                      if proxy in udp_results:
                          e['udp'] = udp_results[proxy]

                  print(f'  UDP: {udp_supported_count} supported, {udp_not_supported} not supported, {udp_errors} errors ({udp_elapsed:.1f}s)')
              except ImportError as ie:
                  print(f'  UDP testing module not available: {ie}')
                  udp_supported_count = 0
              except Exception as e:
                  print(f'  UDP testing failed: {e}')
                  udp_supported_count = 0

              # Save results
              with open('proxies/proxies_working.txt', 'w') as f:
                  for r in working:
                      f.write(f'{r.proxy}\n')

              with open('proxies/proxies_valid.txt', 'w') as f:
                  for r in valid:
                      f.write(f'{r.proxy}\n')

              with open('proxies/results.json', 'w') as f:
                  json.dump(dict(
                      timestamp=datetime.utcnow().isoformat() + 'Z',
                      stats=dict(
                          total=results.total,
                          valid=len(valid),
                          working=len(working),
                          geo_enriched=geo_count,
                          geo_offline=offline_count,
                          geo_api=api_count,
                          threat_checked=threat_count,
                          udp_supported=udp_supported_count,
                          scan_time_sec=scan_time,
                      ),
                      proxies=enriched
                  ), f, indent=2, default=str)

              print(f'Done! Total time: {(datetime.now() - start_time).total_seconds():.1f}s')

          asyncio.run(main())
          "
        env:
          CONCURRENCY: ${{ github.event.inputs.concurrency || '500' }}
          OTX_API_KEY: ${{ secrets.OTX_API_KEY }}

      - name: Generate summary
        working-directory: socks5-scanner
        continue-on-error: true
        run: |
          echo "## Proxy Scan Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          if [ -f proxies/results.json ]; then
            TOTAL=$(python -c "import json; d=json.load(open('proxies/results.json')); print(d.get('stats',{}).get('total', 0))" 2>/dev/null || echo "0")
            VALID=$(python -c "import json; d=json.load(open('proxies/results.json')); print(d.get('stats',{}).get('valid', 0))" 2>/dev/null || echo "0")
            WORKING=$(python -c "import json; d=json.load(open('proxies/results.json')); print(d.get('stats',{}).get('working', 0))" 2>/dev/null || echo "0")
            GEO=$(python -c "import json; d=json.load(open('proxies/results.json')); print(d.get('stats',{}).get('geo_enriched', 0))" 2>/dev/null || echo "0")
            GEO_OFFLINE=$(python -c "import json; d=json.load(open('proxies/results.json')); print(d.get('stats',{}).get('geo_offline', 0))" 2>/dev/null || echo "0")
            GEO_API=$(python -c "import json; d=json.load(open('proxies/results.json')); print(d.get('stats',{}).get('geo_api', 0))" 2>/dev/null || echo "0")
            THREAT=$(python -c "import json; d=json.load(open('proxies/results.json')); print(d.get('stats',{}).get('threat_checked', 0))" 2>/dev/null || echo "0")
            UDP=$(python -c "import json; d=json.load(open('proxies/results.json')); print(d.get('stats',{}).get('udp_supported', 0))" 2>/dev/null || echo "0")
            TIME=$(python -c "import json; d=json.load(open('proxies/results.json')); print(f\"{d.get('stats',{}).get('scan_time_sec', 0):.0f}s\")" 2>/dev/null || echo "?")
            echo "| Metric | Count |" >> $GITHUB_STEP_SUMMARY
            echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
            echo "| Total Scanned | $TOTAL |" >> $GITHUB_STEP_SUMMARY
            echo "| SOCKS5 Valid | $VALID |" >> $GITHUB_STEP_SUMMARY
            echo "| Fully Working | $WORKING |" >> $GITHUB_STEP_SUMMARY
            echo "| GeoIP Enriched | $GEO |" >> $GITHUB_STEP_SUMMARY
            echo "| ↳ Offline (GeoLite2) | $GEO_OFFLINE |" >> $GITHUB_STEP_SUMMARY
            echo "| ↳ API (ip-api.com) | $GEO_API |" >> $GITHUB_STEP_SUMMARY
            echo "| Threat Checked | $THREAT |" >> $GITHUB_STEP_SUMMARY
            echo "| **UDP Supported** | **$UDP** |" >> $GITHUB_STEP_SUMMARY
            echo "| Scan Time | $TIME |" >> $GITHUB_STEP_SUMMARY
          else
            echo "No results file found" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Commit results
        working-directory: socks5-scanner
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          git add proxies/ || true
          git diff --staged --quiet || git commit -m "Update proxy list [skip ci]"
          git push || true
